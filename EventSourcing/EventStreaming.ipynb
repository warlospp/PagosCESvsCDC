{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ba60db-23f6-4aaf-8481-67b857781c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col, from_json, regexp_replace, to_timestamp, lit, count, max, avg, min\n",
    "# -----------------------------------------------\n",
    "# STEP 1: Definir esquemas\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Esquema para los datos planos dentro de 'current'\n",
    "pago_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType()),\n",
    "    StructField(\"ClienteId\", StringType()),\n",
    "    StructField(\"Monto\", StringType()),  # money como string para evitar problemas\n",
    "    StructField(\"MetodoPago\", StringType()),\n",
    "    StructField(\"FechaPago\", StringType()),  # datetime como string para parsear luego\n",
    "    StructField(\"Estado\", StringType())\n",
    "])\n",
    "\n",
    "# Esquema para el CloudEvent recibido\n",
    "cloud_event_schema = StructType([\n",
    "    StructField(\"specversion\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"source\", StringType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"time\", StringType()),\n",
    "    StructField(\"datacontenttype\", StringType()),\n",
    "    StructField(\"operation\", StringType()),\n",
    "    StructField(\"data\", StringType())  # JSON anidado como string\n",
    "])\n",
    "\n",
    "# Esquema para el campo 'data' dentro del CloudEvent\n",
    "data_schema = StructType([\n",
    "    StructField(\"eventsource\", StructType([\n",
    "        StructField(\"db\", StringType()),\n",
    "        StructField(\"schema\", StringType()),\n",
    "        StructField(\"tbl\", StringType()),\n",
    "        StructField(\"cols\", ArrayType(StructType([\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"type\", StringType()),\n",
    "            StructField(\"index\", StringType())\n",
    "        ]))),\n",
    "        StructField(\"pkkey\", ArrayType(StructType([\n",
    "            StructField(\"columnname\", StringType()),\n",
    "            StructField(\"value\", StringType())\n",
    "        ]))),\n",
    "        StructField(\"transaction\", StructType([\n",
    "            StructField(\"commitlsn\", StringType()),\n",
    "            StructField(\"beginlsn\", StringType()),\n",
    "            StructField(\"sequencenumber\", StringType()),\n",
    "            StructField(\"committime\", StringType())\n",
    "        ]))\n",
    "    ])),\n",
    "    StructField(\"eventrow\", StructType([\n",
    "        StructField(\"old\", StringType()),\n",
    "        StructField(\"current\", StringType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Esquema para el mensaje CDC de Confluent Cloud (ajustado al ejemplo)\n",
    "cdc_schema = StructType([\n",
    "    StructField(\"before\", StringType()),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"Id\", IntegerType()),\n",
    "        StructField(\"ClienteId\", StringType()),\n",
    "        StructField(\"Monto\", StringType()),         # Está codificado en Base64, manejar según necesidad\n",
    "        StructField(\"MetodoPago\", StringType()),\n",
    "        StructField(\"FechaPago\", StringType()),     # Timestamp en formato numérico string\n",
    "        StructField(\"Estado\", StringType())\n",
    "    ])),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType()),\n",
    "        StructField(\"connector\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"ts_ms\", LongType()),            # Timestamp en ms de la fuente (inserción)\n",
    "        StructField(\"snapshot\", StringType()),\n",
    "        StructField(\"db\", StringType()),\n",
    "        StructField(\"schema\", StringType()),\n",
    "        StructField(\"table\", StringType()),\n",
    "        StructField(\"commit_lsn\", StringType())\n",
    "    ])),\n",
    "    StructField(\"op\", StringType()),\n",
    "    StructField(\"ts_ms\", LongType()),               # Timestamp en ms del evento en Kafka\n",
    "    StructField(\"transaction\", StringType())\n",
    "])\n",
    "\n",
    "# -----------------------------------------------\n",
    "# STEP 2: Configuración para leer desde Azure Event Hub\n",
    "# -----------------------------------------------\n",
    "\n",
    "event_hub_conn_str = \"Endpoint=sb://arquitecturadatosdemoces.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=Oa8J/Zenf+jtntTTW0CZbgj3dow0WLaz8+AEhDgfUf0=;EntityPath=pagos_ces\"\n",
    "\n",
    "eh_conf = {\n",
    "    'eventhubs.connectionString': sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(event_hub_conn_str)\n",
    "}\n",
    "\n",
    "raw_eh_df = (spark.readStream\n",
    "    .format(\"eventhubs\")\n",
    "    .options(**eh_conf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# STEP 3: Configuración para conectarse a Confluent Cloud Kafka\n",
    "# -----------------------------------------------\n",
    "kafka_bootstrap_servers = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"  # Cambia por tu bootstrap server real\n",
    "kafka_topic = \"bdd_cdc.bdd_cdc.dbo.Pagos\"\n",
    "kafka_api_key = \"T3USLKCFIX4ACKDS\"       # Cambia por tu API key real\n",
    "kafka_api_secret = \"wQP2H5QjtN1geUto7gyBcP3o69ecwItMz7Qfn+HohwUiiDeY6iAuakRoMBt0xA6R\" # Cambia por tu API secret real\n",
    "\n",
    "kafka_conf = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_api_key}\" password=\"{kafka_api_secret}\";',\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "raw_kafka_df = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .options(**kafka_conf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# STEP 4: Decodificación y parseo de los mensajes\n",
    "# -----------------------------------------------\n",
    "# Parsear CloudEvent\n",
    "parsed_eh_df = raw_eh_df.withColumn(\"body_str\", col(\"body\").cast(\"string\"))\n",
    "parsed_eh_df = parsed_eh_df.withColumn(\"cloud_event\", from_json(\"body_str\", cloud_event_schema))\n",
    "# Convertir 'value' de binary a string y Parsear JSON del mensaje Kafka\n",
    "kafka_df = raw_kafka_df.selectExpr(\"CAST(value AS STRING) as json_str\", \"timestamp as kafka_timestamp\")\n",
    "parsed_kafka_df = kafka_df.withColumn(\"cdc\", from_json(col(\"json_str\"), cdc_schema))\n",
    "\n",
    "# Parsear campo 'data' JSON anidado\n",
    "parsed_eh_df = parsed_eh_df.withColumn(\"data_json\", from_json(col(\"cloud_event.data\"), data_schema))\n",
    "# Desescapar el string JSON en 'eventrow.old' para poder parsearlo correctamente\n",
    "clean_eh_current = regexp_replace(col(\"data_json.eventrow.current\"), r'\\\\', '')\n",
    "# Parsear el JSON limpio con el esquema final\n",
    "final_eh_df = parsed_eh_df.withColumn(\"current_data\", from_json(clean_eh_current, pago_schema))\n",
    "# 3. Seleccionar los campos deseados y convertir tipos\n",
    "result_eh_df = final_eh_df.select(\n",
    "    col(\"current_data.Id\").cast(IntegerType()).alias(\"Id\"),\n",
    "    col(\"current_data.ClienteId\").alias(\"ClienteId\"),\n",
    "    col(\"current_data.Monto\").cast(\"double\").alias(\"Monto\"),\n",
    "    col(\"current_data.MetodoPago\"),\n",
    "    to_timestamp(col(\"current_data.FechaPago\"), \"yyyy-MM-dd HH:mm:ss.SSSSSSS\").alias(\"FechaPago\"),\n",
    "    col(\"current_data.Estado\"),\n",
    "    col(\"cloud_event.time\").alias(\"eventhub_time\"),\n",
    "    col(\"enqueuedTime\"),\n",
    "    lit(\"eventhub\").alias(\"source\")\n",
    ")\n",
    "\n",
    "# Extraer campos relevantes\n",
    "processed_kafka_df = parsed_kafka_df.select(\n",
    "    col(\"cdc.after.Id\").alias(\"Id\"),\n",
    "    col(\"cdc.after.ClienteId\").alias(\"ClienteId\"),\n",
    "    # Si 'Monto' está en Base64, decodificarlo si es necesario, aquí se deja como string\n",
    "    col(\"cdc.after.Monto\").alias(\"Monto\"),\n",
    "    col(\"cdc.after.MetodoPago\").alias(\"MetodoPago\"),\n",
    "    # Convertir FechaPago de string numérico a timestamp (nanosegundos desde epoch)\n",
    "    # Ajustar según formato real, aquí se convierte de nanosegundos a timestamp\n",
    "    (col(\"cdc.after.FechaPago\").cast(\"long\") / 1e9).alias(\"FechaPagoEpochSec\"),\n",
    "    col(\"cdc.after.Estado\").alias(\"Estado\"),\n",
    "    col(\"cdc.source.ts_ms\").alias(\"insercion_ts_ms\"),\n",
    "    col(\"cdc.ts_ms\").alias(\"evento_ts_ms\"),\n",
    "    col(\"kafka_timestamp\").alias(\"kafka_ingest_time\"),\n",
    "    lit(\"confluent\").alias(\"source\")\n",
    ")\n",
    "\n",
    "\n",
    "# Convertir FechaPagoEpochSec a timestamp\n",
    "processed_kafka_df = processed_kafka_df.withColumn(\"FechaPago\", to_timestamp(col(\"FechaPagoEpochSec\")))\n",
    "# Calcular diferencia de tiempo en segundos entre inserción y llegada al tópico\n",
    "processed_kafka_df = processed_kafka_df.withColumn(\"tiempo_diferencia_segundos\",(col(\"evento_ts_ms\")/1000 - col(\"insercion_ts_ms\")/1000))\n",
    "# Seleccionar columnas finales\n",
    "result_kafka_df = processed_kafka_df.select(\n",
    "    \"Id\", \"ClienteId\", \"Monto\", \"MetodoPago\", \"FechaPago\", \"Estado\",\n",
    "    \"insercion_ts_ms\", \"evento_ts_ms\", \"kafka_ingest_time\", \"source\", \"tiempo_diferencia_segundos\"\n",
    ")\n",
    "result_kafka_df = result_kafka_df.filter(col(\"FechaPago\").isNotNull())\n",
    "\n",
    "# 4. Calcular diferencia de tiempo\n",
    "result_eh_df = result_eh_df.withColumn(\n",
    "    \"tiempo_diferencia_segundos\",\n",
    "    (col(\"enqueuedTime\").cast(\"long\") - col(\"FechaPago\").cast(\"long\"))\n",
    ")\n",
    "result_eh_df = result_eh_df.filter(col(\"FechaPago\").isNotNull())\n",
    "# Mostrar resultado\n",
    "#result_eh_df.display()\n",
    "# Mostrar resultados (o escribir en Delta Lake)\n",
    "#result_kafka_df.display()\n",
    "\n",
    "# Columnas que tiene result_eh_df pero no result_kafka_df\n",
    "cols_eh = set(result_eh_df.columns)\n",
    "cols_kafka = set(result_kafka_df.columns)\n",
    "# Columnas faltantes en result_kafka_df\n",
    "missing_in_kafka = cols_eh - cols_kafka\n",
    "for c in missing_in_kafka:\n",
    "    result_kafka_df = result_kafka_df.withColumn(c, lit(None))\n",
    "result_kafka_df = result_kafka_df.withColumn(\"kafka_ingest_time_adj\", col(\"kafka_ingest_time\") - expr(\"INTERVAL 0 HOURS\"))\n",
    "result_kafka_df = result_kafka_df.withColumn(\n",
    "    \"tiempo_diferencia_segundos\",\n",
    "    (col(\"kafka_ingest_time_adj\").cast(\"long\") - col(\"FechaPago\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "result_kafka_df = result_kafka_df.drop(\"evento_ts_ms\", \"insercion_ts_ms\", \"Id\", \"kafka_ingest_time\").withColumnRenamed(\"kafka_ingest_time_adj\", \"kafka_ingest_time\")\n",
    "\n",
    "# Columnas faltantes en result_eh_df\n",
    "missing_in_eh = cols_kafka - cols_eh\n",
    "for c in missing_in_eh:\n",
    "    result_eh_df = result_eh_df.withColumn(c, lit(None))\n",
    "result_eh_df = result_eh_df.withColumn(\"enqueuedTime_adj\", col(\"enqueuedTime\") - expr(\"INTERVAL 0 HOURS\"))\n",
    "result_eh_df = result_eh_df.withColumn(\n",
    "    \"tiempo_diferencia_segundos\",\n",
    "    (col(\"enqueuedTime_adj\").cast(\"long\") - col(\"FechaPago\").cast(\"long\"))\n",
    ")# Eliminar columnas no deseadas\n",
    "result_eh_df = result_eh_df.drop(\"evento_ts_ms\", \"insercion_ts_ms\", \"Id\", \"enqueuedTime\").withColumnRenamed(\"enqueuedTime_adj\", \"enqueuedTime\")\n",
    "\n",
    "\n",
    "\n",
    "result_kafka_df = result_kafka_df.select(sorted(result_kafka_df.columns))\n",
    "result_eh_df = result_eh_df.select(sorted(result_eh_df.columns))\n",
    "result_all_df = result_eh_df.unionByName(result_kafka_df)\n",
    "\n",
    "\n",
    "result_all_df = result_all_df.groupBy(\"source\").agg(\n",
    "    count(\"*\").alias(\"numero_registros\"),\n",
    "    max(\"tiempo_diferencia_segundos\").alias(\"duracion_maxima_segundos\"),\n",
    "    avg(\"tiempo_diferencia_segundos\").alias(\"duracion_promedio_segundos\"),\n",
    "    min(\"tiempo_diferencia_segundos\").alias(\"duracion_minima_segundos\")\n",
    ")\n",
    "\n",
    "result_all_df.display()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EventStreaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
